---
title: "Population dynamics of water birds in BR"
author: "Orlando Acevedo-Charry"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  bookdown::html_document2:
    toc: yes
    toc_float: true
    number_sections: yes
    fig_caption: true
    df_print: paged
subtitle: Gompertz Stochastic State-Space population models with Poisson error
editor_options:
  chunk_output_type: console
---

# Intro

**Bhal bhla bhla**. Perhaps a good paper to see is [Lindenmayer *et al.* (2024)](https://onlinelibrary.wiley.com/doi/10.1111/ele.14531) and all those in the subsection "Theory and math" within the "Gompertz State Space model" section.

## Packages

```{r packages}
library(tidyverse)
library(dclone); 
library(mcmcplots)
```

# Data

Update data organized

```{r data}
waterbirds <- read_csv("data/BR_waterbirds_monitoring.csv")
```

Let's convert in a long format to keep monitoring!!

```{r long format of the data}
waterbird_long <- waterbirds |>
  pivot_longer(cols = !c("Original_Date", "Year", "Month", "Day", "Site"), 
               names_to = "Species", 
               values_to = "Count", values_drop_na = TRUE) |>
  mutate(Date = ymd(paste(Year, Month, Day, sep = "-")))

write_csv(waterbird_long, "data/Monitoring_long_format.csv")
```

To this file, I added other columns to include the information in the previous format of monitoring regarding the survey (participants, activity, environmental conditions, hours of sampling) or the birds (e.g., photos, juveniles, flyovers).

Now we can check that each species should have a single count for each time step in the time series in each site. We can select the maximum count per quarter of year per site in each species, as the minimum number of individuals alive in regular steps of observed abundance:

```{r extracting the maximum count per quarter}
waterbird_long <- waterbird_long |> 
  mutate(Date = floor_date(Date, "quarter")) |> 
  dplyr::select(Date, Site, Species, Count) |>
  group_by(Species, Site, Date) |>
  summarise(Count = max(Count)) |>
  as.data.frame()

seq(min(waterbird_long$Date), max(waterbird_long$Date), by = "quarter")
```

The entire time series is 83 quarters of year, from March 2004 to July 2024.

## Summary of the dataset

How many species? How many months of survey per site have each species?
```{r how many species and amount of samples}
waterbird_long |>
  group_by(Species,Site,Date) |>
  count() |> 
  group_by(Species,Site) |>
  count() |> 
  arrange(-n) |> 
  pivot_wider(names_from = "Site", values_from = "n",values_fill = 0) |>
  as.data.frame() 
```

This table is a first result to compare the two sites! Of the 106 species monitored in three sites, we focused on two sites with most data (PET and Embu) and species with counts in at least 10 surveys (time-series length), which is the mean of time steps per species x site combinations. Let's save this in `wtb_ts`, combine to the `waterbird_long` and remove those that do not have ≥10 time steps (identified by NA in the new column `n.time`.

```{r water birds time series}
wtb_ts <- waterbird_long |>
  group_by(Species,Site,Date) |>
  count() |> 
  group_by(Species,Site) |>
  count() |>
  filter(n >= 25) |> 
  rename(n.time = n) |>
  arrange(-n.time) |>
  as.data.frame()
wtb_ts

unique(wtb_ts$Species)

wtb_tsSS <- waterbird_long |>
  left_join(wtb_ts) |>
  filter(!is.na(n.time)) |>
  group_by(Site, Species) |>
  arrange(Date) |>
  as.data.frame()

head(wtb_tsSS)
tail(wtb_tsSS)
```

Now we have the observed time series for the species with better data. Let see the counts of the top-5 species with more data

```{r simple figure of the observed counts}
wtb_tsSS |>
  filter(Species %in% c("Himantopus mexicanus melanurus",
                        "Gallinula galeata",
                        "Amazonetta brasiliensis",
                        "Jacana jacana",
                        "Tringa flavipes")) |>
ggplot(aes(x = Date, y = Count, color = Species)) +
  facet_wrap(factor(Species,
                    levels = c("Himantopus mexicanus melanurus",
                        "Gallinula galeata",
                        "Amazonetta brasiliensis",
                        "Jacana jacana",
                        "Tringa flavipes"))~Site, 
             scales = "free_y", ncol = 2) +
  geom_segment(aes(y = 0,
                   yend = Count),
               alpha = 0.5)+
  geom_point(alpha = 0.6) +
  theme_bw() +
  theme(legend.position = "none",
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        strip.text.x = element_text(face = "italic"),
        strip.background = element_rect(colour = "black", fill = NA),
        panel.border = element_rect(colour = "black", fill = NA))
```

## Prepare the time series for Data cloning

```{r}
prepare_dc_data_list <- function(df) {
  
  # Create full date sequence
  full_dates <- seq(min(df$Date), max(df$Date), by = "quarter")
  
  # Expand grid of all combinations
  full_grid <- expand.grid(Date = full_dates,
                           Site = unique(df$Site),
                           Species = unique(df$Species),
                           stringsAsFactors = FALSE)

  # Merge and sort
  df_full <- full_grid |>
    left_join(df, by = c("Date", "Site", "Species")) |>
    arrange(Site, Species, Date)
  
  # Extract quarter of the year for grouping
  df_full <- df_full |>
    mutate(Date = floor_date(Date, "quarter"))

  # Nest by Site and Species
  nested <- df_full |>
    group_by(Site, Species) |>
    summarise(ts = list(Count), .groups = "drop") |>
    mutate(name = paste(Site, Species, sep = "_"))
  
  # Create named list of time series
  Y1_list <- set_names(nested$ts, nested$name)
  
  # Filter out time series that are all NA
  Y1_list <- Y1_list[!map_lgl(Y1_list, ~ all(is.na(.x)))]

  # Vector of time series lengths
  Tvec <- map_int(Y1_list, length)

  return(list(
    Y1 = Y1_list,
    Tvec = Tvec
  ))
}

```

```{r warning=FALSE}
dc_data <- prepare_dc_data_list(wtb_tsSS)
str(dc_data)
```

# Gompertz State Space model

## Theory and math

You can see [Dennis *et al.* (1991)](https://esajournals.onlinelibrary.wiley.com/doi/10.2307/1943004), [Dennis & Taper (1994)](https://doi.org/10.2307/2937041), [Dennis & Otten (2000)](https://www.jstor.org/stable/pdf/3803237.pdf), [Dennis *et al.*, (2006)](https://esajournals.onlinelibrary.wiley.com/doi/10.1890/0012-9615%282006%2976%5B323%3Aeddpna%5D2.0.co%3B2), [Dennis & Ponciano (2014)](https://esajournals.onlinelibrary.wiley.com/doi/abs/10.1890/13-1486.1), [Ponciano *et al.* (2018)](https://doi.org/10.1016/j.tpb.2018.04.002), and [Evans *et al.* (2022)](https://onlinelibrary.wiley.com/doi/full/10.1111/ibi.13105) for similar models. In addition, interesting readings about State-Space models for population ecology include [Newman *et al.* (2014)](https://doi.org/10.1007/978-1-4939-0977-3), [Newman *et al.* (2023)](https://doi.org/10.1111/2041-210X.13833), or [Auger-Méthe *et al.* (2021)](https://doi.org/10.1002/ecm.1470). We used the Gompertz model because it represent similar estimates of probability of extinction than complex models including Stage or Age-Structure ([Taper *et al.*, 2008](https://link.springer.com/article/10.1007/s11229-007-9299-x)).

The ecological process (State) model for species $i$ in site $m$ (indices not included) is:

$$
N_{t+1} = N_{t}\times\exp\left\{a + b\times\ln\,N_{t} + E_{t} \right\},\\\quad E_{t} \sim \text{Normal}(0,\sigma^2). 
$$
where $N_{t+1}$ is the population size for next time, that came from the previous population size ($N_t$) scaled by the exponential function of a constant that represents intrinsic population growth rate ($a$), another constant that represents the strength of density dependence ($b$) with a no linear relationship with previous population size ($\ln\ N_t$), and the stochasticity of the process ($E_t$), which is a random process assumed to be from a normal distribution and variation $\sigma^2$ (environmental noise).

In the log scale ($X_t = \ln\,N_t$), the model is:

$$
X_{t+1} = X_{t} + a + b\times X_{t} + E_{t},\\\quad E_{t} \sim \text{Normal}(0,\sigma^2). 
$$

or simplified as:

$$
X_{t+1} = a + X_{t}\times (1+b) + E_{t},\\\quad c=1+b ,\\\quad E_{t} \sim \text{Normal}(0,\sigma^2). 
$$

The State-Space model is completed with the observation process, where the observed counts ($Y_t$) are assumed to follow a Poisson distribution with mean $\lambda_t$, which is the exponential of the latent log-abundance state ($X_t$). Thus, the expected count is:

$$
Y_t \sim \text{Poisson}(\lambda_t),\quad \lambda_t = e^{X_t}=N_t
$$

This formulation links the latent (unobserved) population size $N_t$ to the observed data $Y_t$, assuming that counts are noisy realizations of the true abundance.

## Coding the model and test with a single species in a site

Coding the model, including data cloning for diagnostics of estimability of parameters (MLEs) using MCMC methods (see [Lele *et al.*, 2007](https://onlinelibrary.wiley.com/doi/full/10.1111/j.1461-0248.2007.01047.x), [Lele *et al.*, 2010](https://www.math.ualberta.ca/~slele/publications/Lele%20et%20al%20GLMM%202010.pdf), [Sólymos, 2010](https://journal.r-project.org/archive/2010/RJ-2010-011/index.html)):

```{r Stochastic GSS model for data cloning fitting}
StochGSS.dc <- function(){
  
  # Priors on model parameters. Priors are DC1 in Lele et al (2007)
  a1 ~ dnorm(0,1);   # constant, the population growth rate. 
  c1 ~ dunif(-1,1);      # constant, the density dependence parameter. 
  sig1 ~ dlnorm(-0.5,10); #variance parameter of stochastic  environment (process noise) in the system
  stovar1 <- 1/pow(sig1,2)
#  tau1 ~ dunif(0,1); # detection probability (scaling factor that adjust expected counts to imperfect detection or measurement error)
  
  for(k in 1:K){
    # Simulate trajectory that depends on the previous
    mean_X1[1,k] <- a1/(1-c1) # Expected value of the first realization of the process
    # this is drawn from the stationary distribution of the process
    # Equation 14 (main text) and  A.4 in Appendix of Dennis et al 2006
    Varno1[k] <- pow(sig1,2)/(1-pow(c1,2)) #. Equation A.5 in Appendix of Dennis et al 2006
    
    # Updating the state: Stochastic process for all time steps
    X1[1,k]~dnorm(mean_X1[1,k], 1/Varno1[k]); #first estimation of population
    
    #iteration of the GSS model in the data
    for (t in 2:qp1) {
      mean_X1[t,k] <- a1 + c1 * X1[(t - 1),k]
      X1[t,k] ~ dnorm(mean_X1[t,k], stovar1) # Et is included here since a+cX(t-1) + Et ~ Normal(a+cX(t-1),sigma^2)
    }
    
    # Updating the observations, from the counts under Poisson observation error
      # incorporating detection probability directly into the Poisson mean
    for (t in 1:qp1) {
      Y1[t,k] ~ dpois(exp(X1[t,k]));
    }
  }
}
```

Trying to include the detection probability as a scale factor adjusting the expected counts to imperfect detection $Y_t \sim \text{Poisson}(\tau \times \lambda_t)$ resulted in estimability issues, cautioned the issue of including that additional parameter in the model (see [Lele et al. 2010](https://esajournals.onlinelibrary.wiley.com/doi/abs/10.1890/10-0099.1)).

```{r results of the code with tau}
dcrun.GSS.tau <- readRDS("data/test_dcfitGSS_tau.rds")

summary(dcrun.GSS.tau); # see R-hat for tau1 and a1 !!!
dcdiag(dcrun.GSS.tau) # R-hat away of ~1!!!
plot(dcdiag(dcrun.GSS.tau)) # mean squared error and r^2 increasing from 1-10 clones
pairs(dcrun.GSS.tau) # it seems tau is the problem
coef(dcrun.GSS.tau)
```

For this model, we will need some initial values of the parameters, and given that there are many `NA` a wrap to provide those initial guess values.

```{r two functions to guess initial parameters}
guess.calc <- function(Yobs,Tvec){
  
  T.t <-Tvec-Tvec[1]; #  For calculations, time starts at zero.
  q <- length(Yobs)-1;      #  Number of time series transitions, q.
  qp1 <- q+1;              #  q+1 gets used a lot, too.
  S.t <- T.t[2:qp1]-T.t[1:q];  #  Time intervals.
  Ybar <- mean(Yobs);
  Yvar <- sum((Yobs-Ybar)*(Yobs-Ybar))/q;
  mu1 <- Ybar;
  
  # Kludge an initial value for theta based on mean of Y(t+s) given Y(t).
  th1<- -mean(log(abs((Yobs[2:qp1]-mu1)/(Yobs[1:q]-mu1)))/S.t);            
  bsq1<- 2*th1*Yvar/(1+2*th1);         # Moment estimate using stationary
  tsq1<- bsq1;                         #   variance, with betasq=tausq.
  
  #three 0's 
  three0s <- sum(c(th1,bsq1,tsq1))
  if(three0s==0|is.na(three0s)){th1 <- 0.5;bsq1 <- 0.09; tsq1 <- 0.23;}
  
  
  out1 <- c(th1,bsq1,tsq1);
  if(sum(out1<1e-7)>=1){out1 <- c(0.5,0.09,0.23)}
  out <- c(mu1,out1);
  return(abs(out))
  
}

guess.calc2.0<- function(TimeAndNs){
  
  newmat <- TimeAndNs 
  isnas <- sum(is.na(TimeAndNs))
  
  if(isnas >= 1){
    
    isnaind <- which(is.na(TimeAndNs[,2]), arr.ind=TRUE)
    newmat <- TimeAndNs[-isnaind,]
    newmat[,1] <- newmat[,1] - newmat[1,1]
    
  }
  
  init.guess <- guess.calc(Yobs = log(newmat[,2]), Tvec=newmat[,1])
  
  mu1  <- init.guess[1]
  th1  <- init.guess[2]
  bsq1 <- init.guess[3]
  sigsq1<- ((1-exp(-2*th1))*bsq1)/(2*th1)
  
  out <- c(mu=mu1, theta=th1, sigmasq = sigsq1)
  return(out)
}
```

And we have to bundle the data for Data cloning. Let's fit the first species in the taxa with higher data (_Himantopus mexicanus melanurus_ in Embu) as a test.

```{r Fit DC model, eval=FALSE}
ts.4guess  <- dc_data$Y1[[9]] # this is Himantopus mexicanus in Embu
tvec4guess  <- 1:length(ts.4guess)
onets4guess <- cbind(tvec4guess, ts.4guess)
naive.guess <- guess.calc2.0(TimeAndNs = onets4guess)

datalistGSS.dc <- list(K = 1,
                       qp1 = length(ts.4guess),
                       Y1 = dcdim(array(ts.4guess,
                                        dim = c(length(ts.4guess),1)))) 

dcrun.GSS <- dc.fit(data = datalistGSS.dc,
                    params = c("a1", "c1", "sig1"), # previous attempt with tau1
                    model = StochGSS.dc, 
                    n.clones = c(1,5,10,20),
                    multiply = "K",
                    unchanged = "qp1",
                    n.chains = 3,
                    n.adapt = 50000,
                    n.update = 100,
                    thin = 10,
                    n.iter = 100000)

saveRDS(dcrun.GSS, "data/test_dcfitGSS.rds")
```


```{r results of the data cloning fitted model}
dcrun.GSS <- readRDS("data/test_dcfitGSS.rds")

summary(dcrun.GSS);
dcdiag(dcrun.GSS) 
plot(dcdiag(dcrun.GSS))
pairs(dcrun.GSS)
coef(dcrun.GSS)
```

And with these coefficients of the model, we can estimate latent population trajectories with the Kalman filter structure. This Kalman filter structure allows simultaneous estimation of latent states for observed time steps and prediction for missing ones, leveraging the temporal correlation in the process model.

```{r Kalman trajectories model}
Kalman.pred.fn <- function() {
# Priors on model parameters: they are on the real line.
  parms ~ dmnorm(MuPost,PrecPost)
  a1 <- parms[1]   
  c1 <- parms[2] 
  sig1 <- parms[3]
  stovar1 <- 1/pow(sig1,2)

  # Likelihood
  mean_X1[1] <- a1/(1-c1) # Expected value of the first realization of the process
    # this is drawn from the stationary distribution of the process
    # Equation 14 (main text) and  A.4 in Appendix of Dennis et al 2006
    Varno1 <- pow(sig1,2)/(1-pow(c1,2)) #. Equation A.5 in Appendix of Dennis et al 2006
    
    # Updating the state: Stochastic process for all time steps
    X1[1]~dnorm(mean_X1[1], 1/Varno1); #first estimation of population
    N[1] <- exp(X1[1])
    #iteration of the GSS model in the data
    for (t in 2:qp1) {
      mean_X1[t] <- a1 + c1 * X1[(t - 1)]
      X1[t] ~ dnorm(mean_X1[t], stovar1) # Et is included here since a+cX(t-1) + Et ~ Normal(a+cX(t-1),sigma^2)
      Y1[(t-1)] ~ dpois(exp(X1[t]))
      N[t] <- exp(X1[t])
    }
}
```

```{r data for Kalman trajectories}
data4kalman <- list(qp1 = as.numeric(dc_data$Tvec[9]),
                    Y1 = array(dc_data$Y1[[9]],dim = c(as.numeric(dc_data$Tvec[9]))),
                    MuPost = coef(dcrun.GSS),
                    PrecPost = solve(vcov(dcrun.GSS))) 
```

And run the Bayesian inference using the MLE from Data cloning

```{r Fit Bayes Kalman trajectories}
BH_DC_Pred = jags.fit(data=data4kalman, 
                      params=c("N"), 
                      model=Kalman.pred.fn)
```

And we can generate a dataframe of the time series with the estimates, inter quartile range, and observed data through time

```{r Kalman trajectories}
summary(BH_DC_Pred)
# extract predictions and IQR around them
pred <- as.data.frame(t(mcmcapply(BH_DC_Pred, quantile, c(0.25, 0.5, 0.75))))
# generate range of dates for vector of time 
dates_range <- seq(min(wtb_tsSS$Date), max(wtb_tsSS$Date),
                   by = "quarter")

popdyn <- as.data.frame(cbind(dates_range,pred,dc_data$Y1[[9]]))
# modify names 
names(popdyn) <- c("Date", "Lower", "Estimated", "Upper", "Observed")

popdyn |>
  pivot_longer(cols = c(Lower, Estimated, Upper, Observed),
               names_to = "Abundance",
               values_to = "Count") |> 
  ggplot(aes(x = Date, 
             y = Count, 
             color = factor(Abundance,
                            levels = c("Observed",
                                       "Upper",
                                       "Estimated",
                                       "Lower")))) +
    geom_line(aes(linetype = factor(Abundance,
                            levels = c("Observed",
                                       "Upper",
                                       "Estimated",
                                       "Lower")))) +
    geom_point(aes(shape = factor(Abundance,
                            levels = c("Observed",
                                       "Upper",
                                       "Estimated",
                                       "Lower")))) +
    labs(title = expression(italic("Himantopus mexicanus melanurus")~"- Embu"),
         x = "Time (quarter-year)",
         y = "Population",
         color = "Abundance",
         linetype = "Abundance",
         shape = "Abundance") +
    scale_linetype_manual(values = c(NA,"dashed","solid","dashed"))+
    scale_shape_manual(values = c(21, NA,NA,NA)) +
    scale_color_manual(values = c("blue","darkgray","black","darkgray")) +
    theme_classic() +
    theme(legend.position = "bottom")
```
